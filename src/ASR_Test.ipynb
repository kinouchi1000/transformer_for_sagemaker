{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3793fc76-0eed-4454-8646-e84444332027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3deea-413f-420d-b563-af8a31b49a75",
   "metadata": {},
   "source": [
    "# frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb36bd4-c813-4027-b52d-f9b201f6ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.frontend.default import DefaultFrontend\n",
    "\n",
    "frontend = DefaultFrontend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51940c3b-60d0-4ec6-8e0d-328d656a6dfe",
   "metadata": {},
   "source": [
    "# Spec Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e660116a-5616-45ac-af16-ce13494d6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.specaug.specaug import SpecAug\n",
    "\n",
    "specaug = SpecAug(freq_mask_width_range=[0,30],time_mask_width_range=[0,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a0719-44e8-48ba-b6f1-9ccdc72ba90f",
   "metadata": {},
   "source": [
    "# normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5b7b1c-203d-4c4a-99a6-d05b49ade0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layers.global_mvn import GlobalMVN\n",
    "\n",
    "normalizer = GlobalMVN(stats_file=\"test_utils/feats_stats.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478e760-c9ef-4a58-8b2a-21c0db80293d",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9909abf8-ba14-49cb-a7ed-ffcde5a3fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.encoder.transformer_encoder import TransformerEncoder\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    input_size = 80, \n",
    "    input_layer=\"conv2d6\", \n",
    "    num_blocks= 18, \n",
    "    output_size=512,\n",
    "    dropout_rate = 0.1,\n",
    "    positional_dropout_rate = 0.1,\n",
    "    attention_dropout_rate= 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007affb7-798b-4e81-a671-c5774d9f4fec",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250ae3a7-31cd-4523-9591-89e68d19c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder.transformer_decoder import  TransformerDecoder\n",
    "\n",
    "decoder = TransformerDecoder(vocab_size=3262, encoder_output_size=512,self_attention_dropout_rate=0.1,src_attention_dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd48451c-db2f-414b-a06b-81b5e4c5ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ctc import CTC\n",
    "\n",
    "ctc = CTC(odim=3262,encoder_output_size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a323297-14bf-4abb-a938-b699ec671a69",
   "metadata": {},
   "source": [
    "# label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1856433-ef04-410d-8bdd-d1eab7578387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd7f27dc-658a-4a8d-a09e-e77b79c2f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.asr_model import ASRModel\n",
    "\n",
    "model = ASRModel(\n",
    "    vocab_size = 3262,\n",
    "    token_list = [\"aaa\",\"bbb\",\"<blank>\"],\n",
    "    frontend = frontend,\n",
    "    specaug = specaug,\n",
    "    normalize = normalizer,\n",
    "    preencoder= None,\n",
    "    encoder = encoder,\n",
    "    postencoder = None,\n",
    "    decoder = decoder,\n",
    "    ctc = ctc,\n",
    "    joint_network = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff9f9e6-5740-4401-b56f-8df0e9c22dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASRModel(\n",
       "  (frontend): DefaultFrontend(\n",
       "    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n",
       "    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n",
       "  )\n",
       "  (specaug): SpecAug(\n",
       "    (time_warp): TimeWarp(window=5, mode=bicubic)\n",
       "    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n",
       "    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n",
       "  )\n",
       "  (normalize): GlobalMVN(stats_file=test_utils/feats_stats.npz, norm_means=True, norm_vars=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Conv2dSubsampling6(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): Linear(in_features=6144, out_features=512, bias=True)\n",
       "        (1): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoders): MultiSequential(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(3262, 512)\n",
       "      (1): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (output_layer): Linear(in_features=512, out_features=3262, bias=True)\n",
       "    (decoders): MultiSequential(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (criterion_att): LabelSmoothingLoss(\n",
       "    (criterion): KLDivLoss()\n",
       "  )\n",
       "  (ctc): CTC(\n",
       "    (ctc_lo): Linear(in_features=512, out_features=3262, bias=True)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c484b-7fb1-46aa-894f-c0b58a45df58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_sagemaker",
   "language": "python",
   "name": "test_sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
